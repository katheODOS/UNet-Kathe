this iteration was trained with the epoch size modified from 5 to 30

dataset is ~6x larger than original, maybe increasing the number of epochs proportional to dataset size will yield performance increase 
  model,
        device,
        epochs: int = 5,
        batch_size: int = 1,
        learning_rate: float = 1e-5,
        val_percent: float = 0.1,
        save_checkpoint: bool = True,
        img_scale: float = 0.5,
        amp: bool = False,
        weight_decay: float = 1e-8,
        momentum: float = 0.999,
        gradient_clipping: float = 1.0,
        checkpoint_dir: str = None,

and the 512x512 RGB tiles have a 256 pixel overlap and were also augmented by the following characteristics:

Check for 2,3,6,12 as second label
Check for 4 as first label

-these underrerpresented classes were then rotated 90, 180, and 270 degrees. 
training set is 3018 images
val set is 761 images. 

notes: the data was stratified such that the percentage of labels between the two classes are representative of all data. 